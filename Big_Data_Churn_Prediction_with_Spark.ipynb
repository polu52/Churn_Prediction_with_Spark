{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Big Data Churn Prediction with Spark"
      ],
      "metadata": {
        "id": "i4YZi_EloGAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we will develop a big data classification model using the Spark library to predict customer churn.\n",
        "\n",
        "Customer churn refers to the phenomenon where customers stop doing business with a company or cancel a service. Itâ€™s a critical metric for businesses as high churn rates indicate customer dissatisfaction or better alternatives in the market, impacting a company's revenue and growth potential. Churn prediction models aim to identify which customers are likely to leave so that companies can take proactive steps to retain them."
      ],
      "metadata": {
        "id": "B_QmZEptzZ4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src= 'https://miro.medium.com/v2/resize:fit:1400/1*TgciopaOk-C8fwtPmmet3w.png' >"
      ],
      "metadata": {
        "id": "nLfHs_02hOH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries and Dataset"
      ],
      "metadata": {
        "id": "adKiEKqMbQ7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "metadata": {
        "id": "9xOq_l7rnHFQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize Spark\n",
        "spark = SparkSession.builder.appName(\"ChurnClassification\").getOrCreate()"
      ],
      "metadata": {
        "id": "vQFpJQeoxGQy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load the dataset\n",
        "data = spark.read.csv(\"churn.csv\", inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "MMmRs8e8a7AZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdnT3_1GY_Ds",
        "outputId": "71c36d2e-4c44-4b2d-bf31-7f72665bbc86"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+----+--------------+---------------+-----+---------+-----+\n",
            "|_c0|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|Churn|\n",
            "+---+----------------+----+--------------+---------------+-----+---------+-----+\n",
            "|  0|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|    1|\n",
            "|  1|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|    1|\n",
            "|  2|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|    1|\n",
            "|  3|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|    1|\n",
            "|  4|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|    1|\n",
            "+---+----------------+----+--------------+---------------+-----+---------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col in data.columns:\n",
        "     data.describe([col]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kcAFmd4ZCQN",
        "outputId": "e6708afb-22ae-4690-dda8-bf6b8b1eab38"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+\n",
            "|summary|               _c0|\n",
            "+-------+------------------+\n",
            "|  count|               900|\n",
            "|   mean|             449.5|\n",
            "| stddev|259.95191863111916|\n",
            "|    min|                 0|\n",
            "|    max|               899|\n",
            "+-------+------------------+\n",
            "\n",
            "+-------+-------------+\n",
            "|summary|        Names|\n",
            "+-------+-------------+\n",
            "|  count|          900|\n",
            "|   mean|         NULL|\n",
            "| stddev|         NULL|\n",
            "|    min|   Aaron King|\n",
            "|    max|Zachary Walsh|\n",
            "+-------+-------------+\n",
            "\n",
            "+-------+-----------------+\n",
            "|summary|              Age|\n",
            "+-------+-----------------+\n",
            "|  count|              900|\n",
            "|   mean|41.81666666666667|\n",
            "| stddev|6.127560416916251|\n",
            "|    min|             22.0|\n",
            "|    max|             65.0|\n",
            "+-------+-----------------+\n",
            "\n",
            "+-------+-----------------+\n",
            "|summary|   Total_Purchase|\n",
            "+-------+-----------------+\n",
            "|  count|              900|\n",
            "|   mean|10062.82403333334|\n",
            "| stddev|2408.644531858096|\n",
            "|    min|            100.0|\n",
            "|    max|         18026.01|\n",
            "+-------+-----------------+\n",
            "\n",
            "+-------+------------------+\n",
            "|summary|   Account_Manager|\n",
            "+-------+------------------+\n",
            "|  count|               900|\n",
            "|   mean|0.4811111111111111|\n",
            "| stddev|0.4999208935073339|\n",
            "|    min|                 0|\n",
            "|    max|                 1|\n",
            "+-------+------------------+\n",
            "\n",
            "+-------+-----------------+\n",
            "|summary|            Years|\n",
            "+-------+-----------------+\n",
            "|  count|              900|\n",
            "|   mean| 5.27315555555555|\n",
            "| stddev|1.274449013194616|\n",
            "|    min|              1.0|\n",
            "|    max|             9.15|\n",
            "+-------+-----------------+\n",
            "\n",
            "+-------+------------------+\n",
            "|summary|         Num_Sites|\n",
            "+-------+------------------+\n",
            "|  count|               900|\n",
            "|   mean| 8.587777777777777|\n",
            "| stddev|1.7648355920350969|\n",
            "|    min|               3.0|\n",
            "|    max|              14.0|\n",
            "+-------+------------------+\n",
            "\n",
            "+-------+-------------------+\n",
            "|summary|              Churn|\n",
            "+-------+-------------------+\n",
            "|  count|                900|\n",
            "|   mean|0.16666666666666666|\n",
            "| stddev| 0.3728852122772358|\n",
            "|    min|                  0|\n",
            "|    max|                  1|\n",
            "+-------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML- Classification"
      ],
      "metadata": {
        "id": "OIDu-pjabb1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Prepare the data for training\n",
        "feature_columns = [col for col in data.columns if col not in [\"Names\", \"Churn\"]]\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"Features\")\n",
        "data = assembler.transform(data).select(\"Features\", \"Churn\")\n"
      ],
      "metadata": {
        "id": "dk7luBrXZO5e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Split the data into training and testing sets\n",
        "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)"
      ],
      "metadata": {
        "id": "40EyD57nZ_oP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train a logistic regression model\n",
        "gbt = GBTClassifier(labelCol=\"Churn\", featuresCol=\"Features\")\n",
        "model = gbt.fit(train_data)"
      ],
      "metadata": {
        "id": "6gap7cfqaHyV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Make predictions on the test data\n",
        "predictions = model.transform(test_data)"
      ],
      "metadata": {
        "id": "Y30HJuOoawH2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Evaluate the model\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Churn\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoIoK01dayAd",
        "outputId": "8a0f7311-9f44-43b9-ffc6-1025783d6582"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9973262032085561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "qVOEXARCbqKe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion"
      ],
      "metadata": {
        "id": "iv2kbDfBbsJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we developed a customer churn prediction model on big data using the Spark library. Leveraging Spark's powerful data processing capabilities, we analyzed large datasets, utilized VectorAssembler for vector transformations, and applied the GBTClassifier (Gradient Boosted Trees) algorithm as our classification model. We evaluated the model's performance with BinaryClassificationEvaluator for accuracy measurement. Through this project, we successfully built a model capable of efficiently predicting churn on large-scale datasets, harnessing Spark's scalable data processing framework."
      ],
      "metadata": {
        "id": "neHMKZkQegmF"
      }
    }
  ]
}